# ML_DL_Interview

##a Understnding Metrics like - Accuracy, ROC, AUC, Precision, Recall, Sensitivity, Specificty, True Positive Rate, False Positive Rate

What is Precision and Recall?

What is ROC Curve?

What is AUC?

What is the best value for ROC curve?

What is Precision-Recall Curve?

What is the best value for Precision-Recall Curve?

What is Senitivity and Specificity?

When to use ROC curve and when to use precision-recall curve?

which is the better curve for interpreting model performance when you have imbalance dataset ROC or Precision-recall and why?

What is ACcuracy?

In case of imbalance dataset , does accuracy serve to be a better metric for model evaluation, if not why?

Links:
https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/

https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5

https://towardsdatascience.com/receiver-operating-characteristic-curves-demystified-in-python-bd531a4364d0

http://www.navan.name/roc/

https://towardsdatascience.com/confused-by-the-confusion-matrix-e26d5e1d74eb
